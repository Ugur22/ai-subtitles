# ðŸŽ¤ Speaker Diarization Setup Guide

Speaker diarization has been successfully integrated into your AI Subtitles application! This feature automatically identifies different speakers in your videos and labels them with colors, so you can see **who says what**.

## âœ… What's Been Implemented

### Backend Changes:
- âœ… Speaker diarization module (`speaker_diarization.py`) using pyannote.audio
- âœ… Integration into both transcription endpoints (`/transcribe/` and `/transcribe_local/`)
- âœ… Automatic speaker detection and labeling
- âœ… Speaker information stored in database with transcriptions
- âœ… Color-coded speaker display

### Frontend Changes:
- âœ… Speaker badges with unique colors for each speaker
- âœ… Speaker labels formatted nicely (SPEAKER_00 â†’ Speaker 1, etc.)
- âœ… Visual indicators with user icons
- âœ… Consistent color coding across all segments

---

## ðŸ“‹ Setup Instructions

### Step 1: Get a Hugging Face Token

1. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Click "New token"
3. Name it "pyannote-speaker-diarization"
4. Select **Read** access
5. Click "Generate token"
6. **Copy the token** (you'll need it in the next step)

### Step 2: Accept Pyannote License

1. Go to [https://huggingface.co/pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
2. Click "Agree and access repository"
3. Also accept the license at [https://huggingface.co/pyannote/segmentation-3.0](https://huggingface.co/pyannote/segmentation-3.0)

### Step 3: Configure Environment Variables

Create or update your `.env` file in the `backend` directory:

```bash
cd backend

# Create .env file
cat > .env << 'EOF'
# Hugging Face Token for Speaker Diarization
HUGGINGFACE_TOKEN=your_token_here

# Enable/Disable Speaker Diarization
ENABLE_SPEAKER_DIARIZATION=true

# Speaker Detection Settings
MIN_SPEAKERS=1
MAX_SPEAKERS=10

# Whisper Model Settings (existing)
FASTWHISPER_MODEL=small
FASTWHISPER_DEVICE=cpu
FASTWHISPER_COMPUTE_TYPE=int8
EOF
```

**Replace `your_token_here` with your actual Hugging Face token!**

### Step 4: Restart the Backend

```bash
# Make sure you're in the backend directory
cd backend

# Activate virtual environment if needed
source venv/bin/activate  # On macOS/Linux
# or
venv\Scripts\activate  # On Windows

# Restart the server
uvicorn main:app --reload
```

You should see:
```
Speaker diarization module initialized successfully
```

---

## ðŸŽ¬ How to Use

### 1. Upload a Video with Multiple Speakers

Upload any video with dialogue or conversation (e.g., interview, movie scene, podcast).

### 2. Watch the Transcription Process

In the backend console, you'll see:

```
============================================================
ðŸŽ¤ Starting speaker diarization...
============================================================
Performing speaker diarization on: /path/to/audio.wav
Diarization complete. Found 2 speakers in 45 segments
Assigning speakers to 120 transcription segments...
Speaker assignment complete. Identified 2 unique speakers

============================================================
âœ… Speaker diarization complete!
Found 2 unique speakers:
  - Speaker 1: 65 segments
  - Speaker 2: 55 segments
============================================================
```

### 3. View Results in Frontend

Each transcript segment will now show:
- **Colored speaker badge** (e.g., "Speaker 1" in violet, "Speaker 2" in rose)
- **User icon** next to the speaker name
- **Consistent colors** throughout the transcript

Example display:
```
[ðŸŽ¬ 00:00:05] â†’ 00:00:08 | [ðŸ‘¤ Speaker 1] | Segment 1
"Hello, how are you doing today?"

[ðŸŽ¬ 00:00:09] â†’ 00:00:12 | [ðŸ‘¤ Speaker 2] | Segment 2
"I'm doing great, thanks for asking!"
```

---

## ðŸŽ¨ Speaker Colors

The system automatically assigns consistent colors to speakers:
- **Speaker 1**: Violet
- **Speaker 2**: Rose
- **Speaker 3**: Emerald
- **Speaker 4**: Amber
- **Speaker 5**: Cyan
- **Speaker 6**: Pink
- **Speaker 7**: Purple
- **Speaker 8**: Teal

---

## âš™ï¸ Configuration Options

### Adjust Number of Expected Speakers

If you know how many speakers are in your video, you can optimize detection:

In `.env`:
```bash
# For a 1-on-1 interview
MIN_SPEAKERS=2
MAX_SPEAKERS=2

# For a panel discussion with 3-5 people
MIN_SPEAKERS=3
MAX_SPEAKERS=5

# For meetings with unknown number of speakers
MIN_SPEAKERS=1
MAX_SPEAKERS=10
```

### Disable Speaker Diarization

If you want to temporarily disable it:

```bash
ENABLE_SPEAKER_DIARIZATION=false
```

---

## ðŸš€ Performance Tips

### GPU Acceleration (Optional but Recommended)

Speaker diarization is **much faster** with a GPU. If you have an NVIDIA GPU:

1. Install CUDA toolkit
2. Install GPU-enabled PyTorch:
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```
3. Update your `.env`:
   ```bash
   FASTWHISPER_DEVICE=cuda
   ```

**Speed improvement**: GPU can be 5-10x faster than CPU!

### Processing Time Estimates

| Video Duration | CPU (8-core) | GPU (RTX 3060) |
|---------------|-------------|----------------|
| 1 minute | ~15 seconds | ~3 seconds |
| 5 minutes | ~1 minute | ~15 seconds |
| 30 minutes | ~6 minutes | ~1.5 minutes |
| 1 hour | ~12 minutes | ~3 minutes |

---

## ðŸ› Troubleshooting

### Issue: "HUGGINGFACE_TOKEN not found"

**Solution**: Make sure your `.env` file is in the `backend` directory and contains:
```bash
HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

### Issue: "Failed to load speaker diarization pipeline"

**Solutions**:
1. Make sure you accepted the pyannote license (see Step 2)
2. Check your token has read access
3. Try running: `pip install --upgrade pyannote.audio`

### Issue: "Speaker diarization is very slow"

**Solutions**:
1. Use GPU acceleration (see Performance Tips)
2. Process shorter videos (under 30 minutes)
3. Reduce MAX_SPEAKERS if you know there are fewer speakers

### Issue: "All segments show same speaker"

**Solutions**:
1. Video might actually have only one speaker
2. Adjust MIN_SPEAKERS and MAX_SPEAKERS in `.env`
3. Check audio quality - poor audio can affect detection

### Issue: "Too many speakers detected"

**Solution**: Set a more restrictive range in `.env`:
```bash
MIN_SPEAKERS=2
MAX_SPEAKERS=3
```

---

## ðŸ“Š Example Output

When you transcribe a video conversation, you'll see:

```
[00:00:01 â†’ 00:00:04] [Speaker 1] Segment 1
Hello everyone, welcome to today's podcast.

[00:00:05 â†’ 00:00:08] [Speaker 2] Segment 2
Thank you for having me, I'm excited to be here!

[00:00:09 â†’ 00:00:15] [Speaker 1] Segment 3
Let's start by talking about your new project.

[00:00:16 â†’ 00:00:22] [Speaker 2] Segment 4
Sure, we've been working on this for about six months now...
```

Each speaker gets a unique color, making it easy to follow the conversation!

---

## ðŸŽ¯ Next Steps (Optional Enhancements)

Want to take it further? Here are some features you can add:

1. **Speaker Renaming**: Rename "Speaker 1" to "John", "Speaker 2" to "Sarah"
2. **Speaker Filtering**: View transcript for only one speaker
3. **Speaker Statistics**: Show speaking time, word count per speaker
4. **Export by Speaker**: Download separate transcripts for each speaker

Let me know if you'd like help implementing any of these!

---

## âœ¨ Summary

You now have a fully functional speaker diarization system that:
- âœ… Automatically detects multiple speakers
- âœ… Labels them with colors
- âœ… Shows who says what in your videos
- âœ… Works with any language (combined with translation)

**Just upload a video and watch the magic happen!** ðŸŽ‰
